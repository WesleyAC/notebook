<!DOCTYPE html>
<html><head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <title>
            Pijul - On fires
            
        </title>
        <meta name="description" content="An easy to use, distributed and fast version control system.">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css" integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
        <link rel="stylesheet" href="/css/pijul.css">
        <link rel="canonical" href="http://pijul.org/">
        <link rel="stylesheet" href="https://code.cdn.mozilla.net/fonts/fira.css">
        <link rel="stylesheet" href="/css/syntax.css">
        <link rel="alternate" type="application/rss+xml" title="Pijul" href="http://pijul.org/index.xml">
        <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx" crossorigin="anonymous"></script>
    </head>
    <body class="d-flex flex-column">
        <div class="container-fluid site-header">
            <div class="container-lg">
                <nav class="navbar navbar-expand-md navbar-dark">
                    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav w-100">
                            <li class="nav-item"><a class="nav-link active" href="/">Home</a></li>
                            <li class="nav-item"><a class="nav-link active" href="/downloads">Downloads</a></li>
                            <li class="nav-item"><a class="nav-link active" href="/manual/introduction.html">Documentation</a></li>
                            <li class="nav-item"><a class="nav-link active" href="/posts">Blog</a></li>
                            <li class="nav-item"><a class="nav-link active" href="/community">Community</a></li>
                            <li class="nav-item"><a class="nav-link active" href="/faq">FAQ</a></li>
                            <li class="nav-item"><a class="nav-link active" href="mailto:contact@pijul.org">Contact us</a></li>

                            <li class="nav-item ml-auto"><a class="nav-link active font-weight-bold" href="https://nest.pijul.com">Free hosting</a></li>
                        </ul>
                    </div>
                </nav>
            </div>
            <div class="text-center text-white p-4">
                <h1 class="display-1">Pijul</h1>
            </div>
        </div>
        <div class="container-lg px-5 contents">

<div class="row justify-content-around">
    <div class="col"> <h1>On fires</h1></div>
    
    
    
</div>
<div style="color:#888">Thursday, June 3, 2021</div>
<div style="color:#888;margin-bottom:2em;">By Pierre-Étienne Meunier</div>
<p>Almost three months ago, a fire started in the datacenter that hosted this website, as well as <a href="https://nest.pijul.com">nest.pijul.com</a>.
In this post, I try to reflect on what that meant for us, and the consequences it had.</p>
<h3 id="about-the-fire">About the fire</h3>
<p>Our servers were hosted in a datacenter run by <a href="https://ovh.com">OVH</a>, the largest cloud provider in Europe. Part of the reason includes the fact that I was expecting the most active users initially to be close to the authors, which turned out to be quite wrong (the most active users of the Nest are based in Latvia, New Zealand, France and the US).</p>
<p>The datacenter that hosted our servers was in Strasbourg, and caught fire in the early morning of March 10th, 2021.
The exact cause of the <a href="https://fr.wikipedia.org/wiki/Incendie_du_centre_de_donn%C3%A9es_d%27OVHcloud_%C3%A0_Strasbourg">OVH fire</a> is still under investigation (as of early June, 2021). It is known to have been caused by a UPS unit which had been serviced the day before the fire, but according to OVH, we don’t really know why the fire wasn’t detected fast enough, or why it expanded so quickly.</p>
<p>OVH is a pretty cheap cloud/hosting provider, and is cheap in many ways: because they’ve been around for more than 20 years as a hosting provider, and then as a “cloud” company, they have accumulated a number of different GUIs and APIs to access their services, several of which still in operation. This leads to a rather unintuitive website. But since that company has a radically geeky spirit, all APIs are easy to access, reasonably well documented, so that writing your own cloud tools is easy, since these APIs are documented reasonably well.</p>
<p>After the fire, their reaction has been widely praised for its transparency, and has also been <a href="https://www.journaldunet.com/web-tech/cloud/1499203-six-choses-qu-ovh-ne-dit-pas-sur-l-incendie-de-ses-datacenters-a-strasbourg/">criticised for some parts of their alleged design</a> (even though videos published by OVH seem to contradict some allegations made in that article).
Some people (including myself) have learned with this accident that most features of datacentres, no matter the company, are trade secrets. Not that this is any surprise, but this is a question many users of “the cloud” have probably never asked.
Moreover, since big insurance money is probably involved, I doubt there’s any way to know exactly what happened before the end of the enquiry. One cool part of their response involved Octave Klaba (OVH’s main shareholder) announcing that their fire safety infrastructure would be open sourced.</p>
<h3 id="what-we-could-have-done-better">What we could have done better</h3>
<p>Given that Pijul and the Nest were both advertised as experimental, and we knew that replicating repositories would need its own protocols and algorithms, we didn’t have any replication in place.
We had offsite backups of the database, mirrored to the cloud using <a href="https://restic.net/">Restic</a> on top of OpenStack, but we didn’t have backups of the repository, for reasons I explain below.</p>
<p>In hindsight, just setting an <code>fcron</code> job calling <code>rsync</code> every day to sync the repositories to my machine wouldn’t have costed much, and would have allowed me to recover faster. The total disk space occupied by all repositories isn’t that big anyway.</p>
<h3 id="what-we-have-now">What we have now</h3>
<p>After the fire, the Nest was unavailable for two weeks, during which I started work on a replication protocol. I also (re)defined the backup strategy.</p>
<h5 id="replication">Replication</h5>
<p>Replication doesn’t replace backups, and serves a different purpose: it allows a service to stay afloat even in the presence of fires and a number of other problems. You can still make mistakes and trash the database, or get hacked, in which cases a backup is still essential.</p>
<p>Since fires and network outages do happen, there’s a whole field of computer science called <em>distributed algorithms</em>, dedicated to making services robust to these events. In our particular case, Pijul is essentially a distributed datastore, which makes it particularly easy to turn into a replicated system. The only thing that was missing is a network protocol layer for a server to inform others about new changes.</p>
<p>This is now quite stable, and has been put in production for a week now. The Nest now has three servers in different geographic locations (France, Québec and Singapore), served behind a Cloudflare proxy. I’ll try to summarise a few design principles at different levels of the stack:</p>
<ul>
<li>
<p>First, the <a href="https://en.wikipedia.org/wiki/CAP_theorem"><strong>CAP theorem</strong></a>, which isn’t particularly surprising, and can also be proved quite easily. A young, ambitious CS student looking forward to solving hard problems might even consider it “trivial”. And it certainly is easy to prove, <em>if stated like that as an exercise</em>. However, solving hard problems isn’t the main goal of research in mathematics and computer science: the main goal is understanding things, and we know that <em>naming</em> things is at least 99% of understanding them.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>In this case, clearly naming desirable properties of a distributed system is indeed a very large fraction of the job, and is incredibly useful at the time of actually implementing a system, since you can clearly identify the properties you’re trying to implement, and from there immediately tell which are possible to get and which are not.</p>
<p>More specifically, in the context of the CAP theorem, network partitions do happen, therefore we have to sacrifice either availability or consistency. This gives two major classes of algorithms: <a href="https://en.wikipedia.org/wiki/Conflict-free_replicated_data_type">CRDTs</a> and <a href="https://en.wikipedia.org/wiki/Blockchain">blockchains</a> sacrify consistency, whereas <a href="https://en.wikipedia.org/wiki/Raft_(algorithm)">Raft</a> and <a href="https://en.wikipedia.org/wiki/Paxos_(computer_science)">Paxos</a> sacrify availability.</p>
<p>Another way to try and get a reasonable solution is called <em>eventual consistency</em>, where a service initially sacrifies consistency, but trusts the internet to not remain partitioned for too long. This can also be seen as a weakening of the partition tolerance property. For example, in the particular case of bitcoin, consistency is restored by electing a leader periodically (every ten minutes or so, by mining), and hoping that the result of the election can be broadcast fast enough to the entire internet before another leader gets elected (meaning before the chain splits). If that fails, bitcoin users agree to use the longest chain, which causes data loss.</p>
<p>In our case, we have one “local” database server on each machine of the cluster, used to store the changes coming from the other machines. Every time a new change is received from a user, we try to send it to other members of the cluster, and keep trying after succeeding for two other machines: these two machines will in turn send it to two of their neighbours, which guarantees exponentially fast propagation. The main difficulty is to deal with all kind of changes, including “meta” changes not modelled by patches (such as channel creation, deletion and renaming), and inverse operations such as unrecording a patch.</p>
</li>
<li>
<p>Then, the technical details of <strong>load balancing</strong>. A naive implementation of load balancing, which you can implement at home on your laptop using <a href="http://www.haproxy.com">HAProxy</a>, is just a proxy in front of multiple servers. This is fine, but with the same cost and number of servers, you can build a much faster website for your users, by spreading your servers geographically. Unfortunately, there is something you can only partially do “at home”: routing the traffic to the server closest to the user. One hack you can do with most domain providers is to host your own DNS servers, setup one per zone, and use <a href="https://en.wikipedia.org/wiki/Anycast#Domain_Name_System">DNS anycast</a> to serve different IP addresses to the requests made by your users to your DNS servers. There are two problems with this:</p>
<ol>
<li>
<p>You should probably not host your own DNS servers, mainly because DNS servers need to be fast and have zero downtime, and for that reason have their own redundancy and network systems, and</p>
</li>
<li>
<p>There is a better way: use IP anycast, which allows multiple servers to share the same IP. This routes the traffic to the “most suitable” server with the IP, and allows the network to re-route the traffic to another server if one server stops responding for one reason or another. The reason you can’t do this “at home” is that individual users of the internet don’t usually have much control over IP routing, whereas network operators, CDNs and cloud companies are the ones organising the network, and can play all sorts of tricks with the IP addresses <a href="https://www.icann.org">ICANN</a> gives them.</p>
<p>With this solution, your servers still have their own individual IP addresses, and you can still SSH to the exact machine you want: only the proxy in front of them shares its IP with many others. Yet another reason to use a proxy is to cache a large fraction of your content, making it much much faster for your users.</p>
</li>
</ol>
<p>After a few tests, including OVH’s load balancer, I decided to use Cloudflare’s load balancer and proxy, which is not only very cheap, but also has many more endpoints than all the other networks I considered. It can also act as a CDN, which allowed us to get a speedup in the Nest’s response times by a factor 30 in the worst regions (the Nest was apparently quite annoying to use from New Zealand, for example).</p>
<p>The only downside is that Cloudflare’s reasonably-priced plans are only useful for HTTP traffic, and can’t proxy IP traffic, which would have been helpful for our SSH host, or even future plans to push and pull patches using QUIC (HTTP 3). We don’t have a proper solution yet: pushing things to the Nest now has to be done via <code>ssh.pijul.com</code> (the manual has been updated accordingly).</p>
</li>
<li>
<p>Finally, <strong>replicating databases</strong> is so essential to operating websites that many solutions exist. The two main open source database servers, MySQL/MariaDB and PostgreSQL, have builtin algorithms to do that. However, they need an extra layer on top of them to organise failover when one of the servers fails. That layer could consist of a leader election protocol (such as Raft) to tell which database server is the leader, and which are the followers, as well as to organise failover when the leader fails.</p>
<p>This is what <a href="https://github.com/zalando/patroni">Patroni</a> does, for example. However, after trying to use Patroni for weeks, and seeing it fail to re-elect a leader many, many times (often in the middle of the night)<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>, leaving my servers unusable, I decided to write my own baby replicator using only one super basic strategy, leveraging the existing streaming replication features as implemented in PostgreSQL 12 (or later), as well as a battle-tested leader election tool called <a href="https://etcd.io">Etcd</a>.</p>
<p>The result is available <a href="https://nest.pijul.com/pmeunier/postrep">here</a>.</p>
</li>
</ul>
<h5 id="backups">Backups</h5>
<p>Before the fire, the backups were done in the same datacentre as the main server, which is a mistake and isn’t even cheaper than using a different datacentre. The database was cloned to my laptop regularly (about once a week), and backed up to that datacenter using <a href="https://restic.net/">Restic</a> on top of OpenStack. The repositories, however, were not backed up, mostly because I thought the repository format could still change and make the backups irrelevant. I had also not thought carefully about replication (there are only so many things one can think about at the same time). These aren’t good reasons, though, since a format change definitely didn’t happen at the same time as the fire, and I could have restarted a server on the same day if I had rsync’ed the repos every day.</p>
<p>The new way is to rsync the repositories and the database to my laptop every day, and from there save it to the cloud with Restic. This should allow me to get back on track quickly if really bad things happen, such as a global outage of all three servers simultaneously, together with a fire in the backup server.</p>
<p>Since fcron jobs are easy to forget, I also have an indicator in my i3 bar showing the latest date at which a backup succeeded.</p>
<h3 id="the-cloud-situation">The cloud situation</h3>
<p>One question raised by the fire was, should we stick with OVH, or change provider? Since this is not the first major outage on OVH’s infrastructure, the question isn’t obvious to answer. Outages are a risk with any provider, they just need to be adequately mitigated.</p>
<h5 id="the-cloud-situation-1">The CLOUD situation</h5>
<p>In our case, we have a number of requirements linked to the fact that we host user-generated content, and we know our users may not want to be subjected to censorship by other countries or organisations. Since 2018, the <a href="https://en.wikipedia.org/wiki/CLOUD_Act">CLOUD act</a> <em>directly</em> enables the US administration to enforce US law onto data stored in any other country, whenever the hosting company is registered in the US. I wrote <em>directly</em> there, because there are also other ways the US could enforce their own laws onto foreign companies. The EU has theoretical ways against that (called for example <em>Blocking statute</em> and <em>Instex</em>), but for some reason they seem designed to be useless, and hence no company is using them.</p>
<h5 id="european-alternatives">European alternatives</h5>
<p>A bunch of competitors to OVH are worth mentioning here, especially in Europe, since <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation">GDPR</a> is one of the strictest data protection regulations in the world, and like the CLOUD act in the other direction, has inspired other countries to adjust their own laws. Some of the largest cloud providers in the EU are <a href="https://hetzner.de">Hetzner</a>, <a href="https://3ds.com">Dassault Systems</a> and <a href="https://scaleway.com">Scaleway</a>.</p>
<p>However, these companies are way too active politically for our needs, being either <a href="https://en.wikipedia.org/wiki/Hetzner#Blocking_%22Novaya_Gazeta%22">directly involved in censorship moves</a> or controlled by the <a href="https://en.wikipedia.org/wiki/Dassault_Aviation">arms industry</a>. Scaleway is a more complex case: their offer is very atractive, but their largest shareholder is involved in politics, to the point of buying a major press outlet. While this isn’t a no-go for most projects, it is enough to tip the balance in our specific case.</p>
<p>Now, this doesn’t mean that these services won’t suit your own projects, just that the Nest can’t really achieve its mission of bringing easy and sound collaboration to the world (“peaceful” and “respectful” shouldn’t even <em>need</em> to be mentioned here), while depending on services as close to politics as these.</p>
<h5 id="outside-of-europe">Outside of Europe</h5>
<p><a href="https://nest.pijul.com/rohan">@rohan</a> is one of the most active contributors to Pijul. He’s done an amazing job on supporting data encodings other than UTF-8 in diffs without treating them as binary, and taught me about the <a href="https://en.wikipedia.org/wiki/Southern_Cross_Cable">Southern Cross Cable</a>, a fascinating piece of Internet infrastructure. The route he used was through that cable across the Pacific ocean, then a route across the US, and finally across the Atlantic ocean and a bit of France. Now he’s using the Singapore server, cached by Cloudflare in Auckland.</p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>The realisation that naming things is a discipline of its own has probably even been one of the greatest discoveries of the 20th century. <a href="https://en.wikipedia.org/wiki/Georg_Cantor">Cantor</a> probably started that, by rebuilding foundations for mathematics (definitions were particularly fuzzy before him), <a href="https://en.wikipedia.org/wiki/Ludwig_Wittgenstein">Wittgenstein</a> established a link with philosophy, blurring the distinction between mathematics and philosophy. <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a> even established a distinction between the scientists who name things (whom he called “revolutionary”) and the others (the “normies”). <a href="https://en.wikipedia.org/wiki/Gilles_Deleuze">Deleuze</a> restated the role of the philosopher as a creator of concepts (which also applies outside of science), or in other words, as a <em>professional namer</em>. And by the way, the history of Computer Science is full of such half-philosophical, half-mathematical discoveries, where naming is almost everything: Turing machines, <a href="https://en.wikipedia.org/wiki/Communication_complexity">Communication Complexity</a>, <a href="https://en.wikipedia.org/wiki/Yao%27s_principle">Yao’s principle</a>…&nbsp;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">↩︎</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>Patroni itself is probably fine, but tries to be extremely generic in its backends to follow all the evolutions in the ecosystem: for example, <code>patronictl</code> worked fine for me for Etcd 2, but I would never see any server in the cluster using Etcd 3.&nbsp;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">↩︎</a></p>
</li>
</ol>
</section>
</div>

<div class="container-lg px-5 py-3 mt-auto  small text-secondary">
    <footer>
        <div class="row justify-content-between">
            <div class="col">
                © 2016-2022 Pierre-Étienne Meunier and Florent Becker
            </div>
            <div class="col-auto">
                <a href="https://twitter.com/pijul_org"><span class="icon icon--twitter"><svg style="width:16px;height:16px" viewBox="0 0 16 16"><path fill="#00ACEE" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path></svg>
                </span><span class="username">pijul_org</span></a>
            </div>

        </div>
    </footer>
</div>


<script defer="" src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;c49831a8da8b406da8b6111aefc1cbc8&quot;}"></script>


</body></html>
