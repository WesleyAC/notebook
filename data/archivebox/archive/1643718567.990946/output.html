<!DOCTYPE html>
<html xmlns:og="http://opengraphprotocol.org/schema/"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A bit about PURLs</title>
  <meta property="og:type" content="article">
  
  
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/css/highlight.css">
  <link rel="canonical" href="https://inkdroid.org/2021/12/16/purl/">
  <link rel="alternate" type="application/rss+xml" title="inkdroid" href="https://inkdroid.org/feed.xml">
  <link rel="pingback" href="https://webmention.io/inkdroid.org/xmlrpc">
  <link rel="webmention" href="https://webmention.io/inkdroid.org/webmention">
</head>


  <body>

    <nav id="page-header" class="navbar navbar-default">
  <div class="container-fluid">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="collapse-menu" aria-expanded="false">
        <span class="sr-only">Toggle Navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">inkdroid</a>
    </div>

    <div class="collapse navbar-collapse" id="collapse-menu">
      <ul class="nav navbar-nav pull-right">
        <li><a href="/about/">About</a></li>
        <li><a href="/tags/">Tags</a></li>
        <li><a rel="me" href="http://pinboard.in/u:edsu">Bookmarks</a></li>
        <li><a class="page-link" rel="me" href="https://www.flickr.com/photos/inkdroid">Photos</a></li>
        <li><a class="page-link" rel="me" href="https://bandcamp.com/edsu">Music</a></li>
        <li><a class="page-link" rel="me" href="https://github.com/edsu">Software</a></li>
        <li><a class="page-link" rel="me" href="https://social.coop/@edsu">Social</a></li>
        <li><a class="page-link" href="/talks/">Talks</a></li>
        <li><a class="page-link" rel="alternate" href="/feed.xml"><img height="20" alt="Feed" src="/images/feed.png"></a>
        </li><li><a href="https://webring.xxiivv.com/#random" target="_blank"><img id="webring" src="https://webring.xxiivv.com/icon.black.svg"></a></li>
      </ul>
    </div>
  </div>
</nav>


    <div id="page-content" class="container">
      <div class="col-md-8 col-md-offset-2">
        <div class="post">

  <header class="post-header">
    <h2 class="post-title">A bit about PURLs</h2>
    <p class="post-meta">
      December 16, 2021 <br>
      
      <a rel="tag" class="tag label label-default" href="/tag/metadata">metadata</a>
      
      <a rel="tag" class="tag label label-default" href="/tag/identifiers">identifiers</a>
      
      <a rel="tag" class="tag label label-default" href="/tag/data">data</a>
      
    </p>
  </header>

  <hr>

  <article class="post-content">
    <p>One of my first jobs after finishing an MLS was as a “metadata librarian” at Old Dominion University. This was back in 1998, so it was fitting that one of my first projects was cleaning up all these newfangled <em>URLs</em> that had started popping up in our online catalog.</p>
<p>Many of these records flowed directly in because ODU, like many libraries around the US, participated in the <a href="https://en.wikipedia.org/wiki/Federal_Depository_Library_Program">Federal Depository Library Program</a>, which meant that we imported MARC records for the <a href="https://en.wikipedia.org/wiki/United_States_Government_Publishing_Office">GPO</a> publications that the library selected. Many of these MARC records included <a href="https://www.oclc.org/bibformats/en/8xx/856.html">856</a> fields which pointed at digital equivalents on the emerging web. Sometimes the publications were <em>only</em> available online, which seemed to be the case for datasets that were no longer being distributed on CD-ROM. In these times of <a href="https://bandcamp.com">Bandcamp</a> getting datasets sent on physical media by the post seems like a feature not a bug right? Maybe some are still distributed this way?</p>
<p>The idea that you could click on some text in the catalog (a link) and read the described document was a revolution in consciousness–when it worked. Unfortunately many of these URLs had broken even in the short time that they were in the catalog. They needed to be either updated or removed so that library patrons didn’t go down 404 dead ends when consulting the catalog.</p>
<p>That’s when I first ran across the idea of the <a href="https://en.wikipedia.org/wiki/Persistent_uniform_resource_locator">Persistent Uniform Resource Locator</a> or PURL. I noticed that some of the URLs were starting to all have the same hostname <em>purl.fdlp.gov</em>. PURLs were <a href="https://library.oclc.org/digital/collection/p267701coll28/id/1839">created</a> by Stuart Weibel and Erik Jul at OCLC in 1995, and were picked up by the GPO in 1997 as a way to mitigate broken URLs. The FDLP and GPO are still using PURLs today.</p>
<p>I guess PURL is the original URL shortener. But it was created not to abbreviate otherwise long and otherwise cumbersome URLs, but to make them more resilient and persistent over time. You could put a PURL into a catalog record and if the URL it pointed to needed to change you changed the redirect on the PURL server, and all the places that pointed to the PURL didn’t need to change. It was a beautifully simple idea, and has influenced other approaches like DOI and Handle. But this simplicity depends on a commitment to keeping the PURL up to date, because:</p>
<blockquote>
<p>… every PID is a service commitment involving at least some sweat equity. <span class="citation" data-cites="Kunze:2018">(<a href="#ref-Kunze:2018" role="doc-biblioref"><strong>Kunze:2018?</strong></a>)</span></p>
</blockquote>
<p>PURLs are only as good as the maintenance work that has gone into updating the underlying URLs when they inevitably change. And in the lucky cases where the underlying URL haven’t changed, all the work that has gone into managing the infrastructure behind that URL namespace in order for that URL to stay the same. Updating to remain the same, with apologies to Wendy Chun.</p>
<p>I was reminded of PURLs recently when I happened to <a href="https://twitter.com/edsu/status/1459502321822482437">notice</a> a link to a broken PURL in some <a href="https://web.resource.org/rss/1.0/">old documentation</a> about RSS 1.0. In this case Carl fixed the PURL not by updating the PURL to point at the correct place, but by putting a copy of the document where the PURL expected it to be.</p>
<p>This got me into looking at how the PURL service worked. As the <a href="https://en.wikipedia.org/wiki/Persistent_uniform_resource_locator">Wikipedia article</a> sketches out, PURLs have had a variety of implementations over the years. From the HTML at purl.fdlp.gov it looks like they are running the Zepheira / 3 Round Stones version–it’s not clear to me where one of those began and the other ended, and if any of that software is open source or not.</p>
<p>In 2016 OCLC and the Internet Archive <a href="https://cdm15003.contentdm.oclc.org/digital/collection/p15003coll6/id/660">announced</a> that the venerable purl.org service had moved to the Internet Archive. In the process the software was completely rewritten (I’ve heard it’s a Python application, but I don’t think the source code has been released). One of the interesting side effects of this rewrite is that the new PURL server uses IA’s backend storage as a kind of database–and that storage is public.</p>
<p>So every PURL namespace is an item in the <a href="https://archive.org/details/purl_collection">PURL Data Collection</a>. This collection contains a couple JSON files that describe the PURL/URL mappings contained in that namespace as well as who maintains it, and a history of its changes. So for example the PURL for the Dublin Core Terms namespace is:</p>
<p><a href="https://purl.org/dc/terms">https://purl.org/dc/terms</a></p>
<p>and this IA item contains the metadata for that namespace:</p>
<p><a href="https://archive.org/details/purl_dc_terms/">https://archive.org/details/purl_dc_terms/</a></p>
<p>which really is a s3 like bucket that contains two JSON files:</p>
<ul>
<li><a href="https://ia601202.us.archive.org/34/items/purl_dc_terms/purl_dc_terms_purl.json">purl_dc_terms_purl.json</a></li>
<li><a href="https://ia601202.us.archive.org/34/items/purl_dc_terms/purl_dc_terms_purl_history.json">purl_dc_terms_purl_history.json</a></li>
</ul>
<p>If you squint a bit the PURL system kind of looks like a static site, in that the data it uses is all sitting in static files in their <em>S3-like</em> storage. Of course it’s not really like a static site because there is a dynamic web application reading these files, doing the redirects, and no doubt caching things. Also these seemingly static files are updated when a PURL is edited. But it’s nice that the data is all in JSON files sitting out there for anyone to look at. In some ways it’s not unlike the approach taken by the <a href="https://github.com/perma-id/w3id.org#readme">w3id</a> project, which is basically to <a href="https://github.com/perma-id/w3id.org">use Git</a> and Apache to version identifier namespaces. This is fanciful, but both approaches kind of remind me of the exposed pipes on the <a href="https://en.wikipedia.org/wiki/Centre_Pompidou">Centre Pompidou</a>, where the color of each pipe <a href="https://www.centrepompidou.fr/en/collections/our-building">represents</a> a different function (air, electricity, water, escalators, etc).</p>
<p><a href="https://commons.wikimedia.org/wiki/File:Centre_Georges-Pompidou_34.jpg"><img src="/images/pompidou.jpg" class="img-responsive"></a></p>
<p>The w3id approach is nice because you can get all the data with <code>git clone</code> on the command line. Doing the same thing with purl.org is possible, but takes a little bit more work because you have to use the Internet Archive API to first get all the items in the <a href="https://archive.org/details/purl_collection">PURL Data Collection</a> and then go and fetch the JSON files for all of those items.</p>
<p>I decided to give this a try over in <a href="https://github.com/edsu/notebooks/blob/master/PURL.ipynb">this</a> Jupyter notebook. Once you have the data it’s possible to ask some questions that weren’t easy to ask before.</p>
<p>How often are new PURL namespaces created?</p>
<p><img src="/images/purl-namespace-creation.png" class="img-responsive"></p>
<p>A few obvious things to note here are that PURLs started in 1995, and we only see 2009 on. Also 2009 is off the charts in terms of the number of namespaces that were created that year. One can only assume that the underlying system was modified then to start recording when the namespace was created. Things look a bit more interesting if you ignore the 2009 backlog.</p>
<p><img src="/images/purl-namespace-creation2.png" class="img-responsive"></p>
<p>Here you can clearly see that more PURL namespaces are being created <em>after</em> the move to the Internet Archive in 2016.</p>
<p>Similarly it’s possible to use the history files to see how often namespaces are being updated <em>after</em> they were created.</p>
<p><img src="/images/purl-namespace-updates.png" class="img-responsive"></p>
<p>This chart tells an opposite story, which is more difficult to read. The number of namespace updates seems to have dropped off drastically after the move to the Internet Archive. But the low bar in 2021 is still 1,021 updates, or like 3 namespace changes per day. I’m not entirely sure what to expect. Perhaps there were systems tied to the PURL system that got disconnected when it moved?</p>
<p>Finally one of the more salient questions to ask of this data is how many of the PURLs still work? This is a complex enough question for research project and not just a quick blog post. Over in <a href="https://github.com/edsu/notebooks/blob/master/PURL.ipynb">the notebook</a> I started by sampling all the target URLs (N=405,637 n=662). The idea of checking each and every URL again was more like a full science project. In the process I noticed that it was oversampling some domains quite a bit like <code>my.yoolib.net</code>. So I tried again, but instead of sampling all the URLs I sampled the PURL namespaces (N=21,894, n=644) and picked a random URL from each PURL namespace. This seemed to work better but still appeared to oversample, with host names like <code>www.olemiss.edu</code> showing up quite a bit. It looks like they might create a new PURL namespace for every finding aid they put up?</p>
<p>Of course, testing whether a URL still works is surprisingly tricky business: the response could be 200 OK but say Not Found, or it could be a totally different page (content drift) <span class="citation" data-cites="Zittrain:2021">(<a href="#ref-Zittrain:2021" role="doc-biblioref">Zittrain, Bowers, &amp; Stanton, 2021</a>)</span>. A server could redirect permanently or temporarily to other pages for lots of reasons. Sites can be temporarily offline, etc. But this didn’t stop me from writing a pretty simplistic checker (recalling my days writing Perl at ODU in the 90s) that allowed me to tally the results for the sample:</p>
<p><img class="img-responsive" src="/images/purls-status-codes.png"></p>
<p>One thing to note here is that <a href="https://docs.python-requests.org/en/master/index.html">requests</a> (the Python HTTP library I used) automatically followed redirects, so that’s why you don’t see any 3XX status codes here. The <code>???</code> results are when a TCP/IP error occurred, which (from just scanning the results in the notebook) look to largely be the result of a DNS failure when the host name no longer can be looked up (DNS).</p>
<p>Perhaps it’s more interesting to look at the status codes over namespace creation time. Here it’s important to ignore 2009 since so many were retrospectively assigned to that bucket. This chart is interactive to make it a bit easier to read.</p>
<iframe width="710" height="700" src="/pages/purl-status-codes.html">
</iframe>
<p>This paints a better picture of the resolvability of the PURLs. One thing I noticed while watching the sampled URLs get checked is that a fair number in the “unknown” category appeared to be the result of SSL certificate errors. So to do this properly it would be good to dive into what’s going on in that category. It might also be good to do stratified sampling by year, so the years can be more easily compared?</p>
<p>Since the Internet Archive also run the <a href="https://web.archive.org">Wayback Machine</a> it might make sense for their PURL service to redirect to archived content when the resource is no longer available? I noticed that the PURL. I noticed that the FDLP PURL service presents a history page for PURLs that have gone dead, or are no longer “traceable”. For example take a look at <a href="https://purl.fdlp.gov/GPO/LPS57016">https://purl.fdlp.gov/GPO/LPS57016</a>. It’s almost like a list of “last known addresses”. Both really might be good places to link to the Wayback Machine when a snapshot exists?</p>
<p>I guess I’ll stop here for now. Maybe this little post will whet someone else’s appetite to looking closer at PURL as a web infrastructure. I think it also is a great example of how <em>exposed pipes</em> are useful when building applications that are meant to be infrastructure. I started out wanting to describe how these PURLs are showing up as data but it ended up going in a few directions at once.</p>
<p>One little bit of PURL administrivia that I’ll leave you with that since the PURLs are all part of a <a href="https://archive.org/details/purl_collection?sort=-addeddate">collection</a> at the Internet Archive they also have an RSS feed you can use to watch as new namespaces are created:</p>
<p><a href="https://archive.org/services/collection-rss.php?collection=purl_collection">https://archive.org/services/collection-rss.php?collection=purl_collection</a></p>
<p>And if you are interested in looking at the full and sample datasets, but don’t want to run the notebook yourself, they are here, with the caveat that they are just a snapshot in time:</p>
<ul>
<li><a href="https://media.githubusercontent.com/media/edsu/notebooks/master/data/purl-namespaces.csv">purl-namespaces.csv</a></li>
<li><a href="https://media.githubusercontent.com/media/edsu/notebooks/master/data/purl-namespace-sample.csv">purl-namespace-sample.csv</a></li>
</ul>
<h3 class="unnumbered" id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography">
<div id="ref-Zittrain:2021" class="csl-entry" role="doc-biblioentry">
Zittrain, J. L., Bowers, J., &amp; Stanton, C. (2021). <em>The Paper of Record Meets an Ephemeral Web: An Examination of Linkrot and Content Drift within The New York Times</em> (SSRN Scholarly Paper No. ID 3833133). Rochester, NY: Social Science Research Network. <a href="https://doi.org/10.2139/ssrn.3833133">https://doi.org/10.2139/ssrn.3833133</a>
</div>
</div>

  </article>

</div>

      </div>
    </div>

    <footer id="page-footer">
  <div class="container-fluid">
  <p class="text-center">
    Unless otherwise noted all the content here is licensed <a rel="license" href="https://creativecommons.org/licenses/by/4.0/">CC-BY</a>
  </p>
  <script src="/js/jquery.min.js"></script>
  <script src="/js/bootstrap.min.js"></script>
  <script src="/js/highlight.min.js"></script>
  <script src="/js/welcome.min.js"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>

  </div>
</footer>


  


</body></html>
