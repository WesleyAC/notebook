<!DOCTYPE html>
<html><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>king - man + woman is queen; but why?</title>
  <meta name="description" content="Intro">

  <style class="anchorjs"></style><link rel="stylesheet" href=" /css/main.css">
  <link rel="canonical" href="https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html">
  <link rel="alternate" type="application/rss+xml" title="Piotr Migdał - blog" href="https://p.migdal.pl /feed.xml">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  <meta content="Piotr Migdał - blog" property="og:site_name">
  
  <meta content="king - man + woman is queen; but why?" property="og:title">
  
  
  <meta content="article" property="og:type">
  
  
  <meta content="Words, vectors, analogies and conceptual metaphors - the linear space of word2vec and GloVe. Or: how to change gender with a vector." property="og:description">
  
  
  <meta content="https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html" property="og:url">
  
  
  <meta content="2017-01-06T18:30:00+00:00" property="article:published_time">
  <meta content="https://p.migdal.pl/about/" property="article:author">
  
  
  <meta content="https://p.migdal.pl/imgs/2017-01-06/queen-julia-vectors-facebook.jpg" property="og:image">
  
  
  
  
  
  
  <meta content="machine-learning" property="article:tag">
  
  <meta content="word2vec" property="article:tag">
  
  

<script src="https://connect.facebook.net/en_US/sdk.js?hash=fcc6e7433493efaf6e12bda6d161113c" async="" crossorigin="anonymous"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css" data-fbcssmodules="css:fb.css.base css:fb.css.dialog css:fb.css.iframewidget css:fb.css.customer_chat_plugin_iframe">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:"lucida grande", tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_dialog_advanced{border-radius:8px;padding:10px}.fb_dialog_content{background:#fff;color:#373737}.fb_dialog_close_icon{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{left:5px;right:auto;top:5px}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{height:100%;left:0;margin:0;overflow:visible;position:absolute;top:-10000px;transform:none;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{background:none;height:auto;min-height:initial;min-width:initial;width:auto}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{clear:both;color:#fff;display:block;font-size:18px;padding-top:20px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .4);bottom:0;left:0;min-height:100%;position:absolute;right:0;top:0;width:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:sticky;top:0}.fb_dialog_content .dialog_header{background:linear-gradient(from(#738aba), to(#2c4987));border-bottom:1px solid;border-color:#043b87;box-shadow:white 0 1px 1px -1px inset;color:#fff;font:bold 14px Helvetica, sans-serif;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:linear-gradient(from(#4267B2), to(#2a4887));background-clip:padding-box;border:1px solid #29487d;border-radius:3px;display:inline-block;line-height:18px;margin-top:3px;max-width:85px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{background:none;border:none;color:#fff;font:bold 12px Helvetica, sans-serif;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0 -1px 0}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://static.xx.fbcdn.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #4a4a4a;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #4a4a4a;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://static.xx.fbcdn.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-position:50% 50%;background-repeat:no-repeat;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}
.fb_mpn_mobile_landing_page_slide_out{animation-duration:200ms;animation-name:fb_mpn_landing_page_slide_out;transition-timing-function:ease-in}.fb_mpn_mobile_landing_page_slide_out_from_left{animation-duration:200ms;animation-name:fb_mpn_landing_page_slide_out_from_left;transition-timing-function:ease-in}.fb_mpn_mobile_landing_page_slide_up{animation-duration:500ms;animation-name:fb_mpn_landing_page_slide_up;transition-timing-function:ease-in}.fb_mpn_mobile_bounce_in{animation-duration:300ms;animation-name:fb_mpn_bounce_in;transition-timing-function:ease-in}.fb_mpn_mobile_bounce_out{animation-duration:300ms;animation-name:fb_mpn_bounce_out;transition-timing-function:ease-in}.fb_mpn_mobile_bounce_out_v2{animation-duration:300ms;animation-name:fb_mpn_fade_out;transition-timing-function:ease-in}.fb_customer_chat_bounce_in_v2{animation-duration:300ms;animation-name:fb_bounce_in_v2;transition-timing-function:ease-in}.fb_customer_chat_bounce_in_from_left{animation-duration:300ms;animation-name:fb_bounce_in_from_left;transition-timing-function:ease-in}.fb_customer_chat_bounce_out_v2{animation-duration:300ms;animation-name:fb_bounce_out_v2;transition-timing-function:ease-in}.fb_customer_chat_bounce_out_from_left{animation-duration:300ms;animation-name:fb_bounce_out_from_left;transition-timing-function:ease-in}.fb_invisible_flow{display:inherit;height:0;overflow-x:hidden;width:0}@keyframes fb_mpn_landing_page_slide_out{0%{margin:0 12px;width:100% - 24px}60%{border-radius:18px}100%{border-radius:50%;margin:0 24px;width:60px}}@keyframes fb_mpn_landing_page_slide_out_from_left{0%{left:12px;width:100% - 24px}60%{border-radius:18px}100%{border-radius:50%;left:12px;width:60px}}@keyframes fb_mpn_landing_page_slide_up{0%{bottom:0;opacity:0}100%{bottom:24px;opacity:1}}@keyframes fb_mpn_bounce_in{0%{opacity:.5;top:100%}100%{opacity:1;top:0}}@keyframes fb_mpn_fade_out{0%{bottom:30px;opacity:1}100%{bottom:0;opacity:0}}@keyframes fb_mpn_bounce_out{0%{opacity:1;top:0}100%{opacity:.5;top:100%}}@keyframes fb_bounce_in_v2{0%{opacity:0;transform:scale(0, 0);transform-origin:bottom right}50%{transform:scale(1.03, 1.03);transform-origin:bottom right}100%{opacity:1;transform:scale(1, 1);transform-origin:bottom right}}@keyframes fb_bounce_in_from_left{0%{opacity:0;transform:scale(0, 0);transform-origin:bottom left}50%{transform:scale(1.03, 1.03);transform-origin:bottom left}100%{opacity:1;transform:scale(1, 1);transform-origin:bottom left}}@keyframes fb_bounce_out_v2{0%{opacity:1;transform:scale(1, 1);transform-origin:bottom right}100%{opacity:0;transform:scale(0, 0);transform-origin:bottom right}}@keyframes fb_bounce_out_from_left{0%{opacity:1;transform:scale(1, 1);transform-origin:bottom left}100%{opacity:0;transform:scale(0, 0);transform-origin:bottom left}}@keyframes slideInFromBottom{0%{opacity:.1;transform:translateY(100%)}100%{opacity:1;transform:translateY(0)}}@keyframes slideInFromBottomDelay{0%{opacity:0;transform:translateY(100%)}97%{opacity:0;transform:translateY(100%)}100%{opacity:1;transform:translateY(0)}}</style></head>

<body>

  <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Piotr Migdał - blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"></path>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"></path>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"></path>
        </svg>
      </a>

      <div class="trigger">
        
        
        
        
        
        
        
        
        
        
        
        <a class="page-link" href="/projects/">Projects</a>
        
        
        
        <a class="page-link" href="/articles/">Articles</a>
        
        
        
        <a class="page-link" href="/publications/">Publications</a>
        
        
        
        <a class="page-link" href="/resume/">Resume</a>
        
        
        
        <a class="page-link" href="/about/">About</a>
        
        
        <a class="page-link" href="http://migdal.zenfolio.com/">Photos</a>
      </div>
    </nav>

  </div>

</header>

  <div class="page-content">
    <div class="wrapper">
      
<script async="" src="//www.google-analytics.com/analytics.js"></script><script id="twitter-wjs" src="https://platform.twitter.com/widgets.js"></script><script id="facebook-jssdk" src="//connect.facebook.net/en_US/sdk.js#xfbml=1&amp;version=v2.5&amp;appId=262622227112392"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>



<article class="post" itemscope="" itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline" id="king-man-woman-is-queen-but-why">king - man + woman is queen; but why?</h1>
    <p class="post-meta">
      <time datetime="2017-01-06T18:30:00+00:00" itemprop="datePublished">6 Jan 2017</time>
      
      • <span itemprop="author" itemscope="" itemtype="http://schema.org/Person"><span itemprop="name">Piotr Migdał</span></span>
      
      •
      
      <span>[machine-learning]</span>
      
      <span>[word2vec]</span>
      
      
      
      <br>
      see: <a href="https://news.ycombinator.com/item?id=13346104">Hacker News discussion thread with over 250 upvotes</a>
      
      
    </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h2 id="intro">Intro<a class="anchorjs-link " href="#intro" aria-label="Anchor link for: intro" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p><strong>word2vec</strong> is an algorithm that transforms words into vectors, so that words with similar meaning end up laying close to each other. Moreover, it allows us to use vector arithmetics to work with analogies, for example the famous <code class="language-plaintext highlighter-rouge">king - man + woman = queen</code>.</p>

<p>I will try to explain how it works, with special emphasis on the meaning of vector differences, at the same time omitting as many technicalities as possible.</p>

<p>If you would rather explore than read, here is an interactive exploration by my mentee Julia Bazińska, now a freshman of computer science at the University of Warsaw:</p>

<ul>
  <li><a href="https://lamyiowce.github.io/word2viz/">Word2viz</a> by using <a href="http://nlp.stanford.edu/projects/glove/">GloVe</a> pre-trained vectors (it takes 30MB to load - please be patient)</li>
</ul>

<p><a href="https://lamyiowce.github.io/word2viz/"><img src="/imgs/2017-01-06/word2viz-queen.png" alt=""></a></p>

<h2 id="counts-coincidences-and-meaning">Counts, coincidences and meaning<a class="anchorjs-link " href="#counts-coincidences-and-meaning" aria-label="Anchor link for: counts coincidences and meaning" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<blockquote>
  <p>I love letter co-occurrence in the word <em>co-occurrence</em>.</p>
</blockquote>

<p>Sometimes a seemingly naive technique gives powerful results. It turns out that merely looking at word coincidences, while ignoring all grammar and context, can provide us insight into the meaning of a word.
Consider this sentence:</p>

<blockquote>
  <p>A small, fluffy roosety climbed a tree.</p>
</blockquote>

<p>What’s a <em>roosety</em>? I would say that something like a squirrel, since the two words can be easily interchanged. Such reasoning is called the <a href="https://en.wikipedia.org/wiki/Distributional_semantics">distributional hypothesis</a> and can be summarized as:</p>

<blockquote>
  <p>a word is characterized by the company it keeps - <a href="https://en.wikipedia.org/wiki/John_Rupert_Firth">John Rupert Firth</a></p>
</blockquote>

<p>If we want to teach it to a computer, the simplest, approximated approach is making it look only at word pairs.
Let <em>P(a|b)</em> be the conditional probability that given a word <em>b</em> there is a word <em>a</em> within a short distance (let’s say - being spaced by no more that 2 words).
Then we claim that two words <em>a</em> and <em>b</em> are similar if</p>

\[P(w|a) = P(w|b)\]

<p>for every word <em>w</em>.
In other words, if we have this equality, no matter if there is word <em>a</em> or <em>b</em>, all other words occur with the same frequency.</p>

<p>Even simple word counts, compared by source, can give interesting results, e.g. that in lyrics of metal songs words (<em>cries</em>, <em>eternity</em> or <em>ashes</em> are popular, while words <em>particularly</em> or <em>approximately</em> are not, well, particularly common), see <a href="http://www.degeneratestate.org/posts/2016/Apr/20/heavy-metal-and-natural-language-processing-part-1/">Heavy Metal and Natural Language Processing</a>.
See also <a href="http://juliasilge.com/blog/Gender-Pronouns/">Gender Roles with Text Mining and N-grams</a> by Julia Silge.</p>

<p>Looking at co-occurrences can provide much more information. For example, one of my projects, <a href="http://p.migdal.pl/tagoverflow/">TagOverflow</a>, gives insight into structure of programming, based only on the usage of <a href="http://stackoverflow.com/tags">tags on Stack Overflow</a>.
It also shows that I am in love with pointwise mutual information, which brings us to the next point.</p>

<p><a href="http://p.migdal.pl/tagoverflow/?site=english&amp;size=32"><img src="/imgs/2017-01-06/word2viz-tagoverflow-english.png" alt=""></a></p>

<h2 id="pointwise-mutual-information-and-compression">Pointwise mutual information and compression<a class="anchorjs-link " href="#pointwise-mutual-information-and-compression" aria-label="Anchor link for: pointwise mutual information and compression" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<blockquote>
  <p>The Compressors: View cognition as compression. Compressed sensing, approximate matrix factorization - <a href="https://news.ycombinator.com/item?id=10954508">The (n) Cultures of Machine Learning - HN discussion</a></p>
</blockquote>

<p>In principle, we can compute <em>P(a|b)</em> for every word pair.
But even with a small dictionary of 100 000 words (bear in mind that we need to keep all declinations, proper names and things which are not in official dictionaries, yet are in use) keeping track of all pairs would require 8 gigabytes of space.</p>

<p>Often instead of working with conditional probabilities, we use the <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information">pointwise mutual information</a> (PMI), defined as:</p>

\[PMI(a, b) = \log \left[ \frac{P(a,b)}{P(a)P(b)} \right] = \log \left[ \frac{P(a|b)}{P(a)} \right].\]

<p>Its direct interpretation is how much more likely we get a pair than if it were <a href="https://en.wikipedia.org/wiki/Independence_(probability_theory)">at random</a>.
The logarithm makes it easier to work with <a href="https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/">words appearing at frequencies</a> of different orders of magnitude.
We can approximate PMI as a scalar product:</p>

\[PMI(a, b) = \vec{v}_a \cdot \vec{v}_b,\]

<p>where \(\vec{v}_i\) are vectors, typically of 50-300 dimensions.</p>

<p>At the first glance it may be strange that all words can be compressed to a space of much smaller dimensionality. But there are words that can be trivially interchanged (e.g. <em>John</em> to <em>Peter</em>) and there is a lot of structure in general.</p>

<p>The fact that this compression is lossy may give it an advantage, as it can discover patterns rather than only memorize each pair.
For example, in recommendation systems for movie ratings, each rating is approximated by a scalar product of two vectors - a movie’s content and a user’s preference. This is used to predict scores for as yet unseen movies, see <a href="http://katbailey.github.io/post/matrix-factorization-with-tensorflow/">Matrix Factorization with TensorFlow - Katherine Bailey</a>.</p>

<h2 id="word-similarity-and-vector-closeness">Word similarity and vector closeness<a class="anchorjs-link " href="#word-similarity-and-vector-closeness" aria-label="Anchor link for: word similarity and vector closeness" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<blockquote>
  <p>Stock Market ≈ Thermometer - <a href="http://byterot.blogspot.com/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html">Five crazy abstractions my word2vec model just did</a></p>
</blockquote>

<p>Let us start with the simple stuff - showing word similarity in a vector space.
The condition that \(P(w \vert a)=P(w \vert b)\) is equivalent to</p>

\[PMI(w, a) = PMI(w, b),\]

<p>by dividing both sides by <em>P(w)</em> and taking their logarithms.
After expressing PMI with vector products, we get</p>

\[\vec{v}_w \cdot \vec{v}_a = \vec{v}_w \cdot \vec{v}_b\]

\[\vec{v}_w \cdot \left( \vec{v}_a - \vec{v}_b \right) = 0\]

<p>If it needs to work for every \(\vec{v}_w\), then</p>

\[\vec{v}_a = \vec{v}_b.\]

<p>Of course, in every practical case we won’t get an exact equality, just words being close to each other. Words close in this space are often synonyms (e.g. <em>happy</em> and <em>delighted</em>), antonyms (e.g. <em>good</em> and <em>evil</em>) or other easily interchangeable words (e.g. <em>yellow</em> and <em>blue</em>).
In particular most of <a href="https://en.wikipedia.org/wiki/Horseshoe_theory">opposing ideas</a> (e.g. <em>religion</em> and <em>atheism</em>) will have similar context. If you want to play with that, look at this <a href="https://demos.explosion.ai/sense2vec/?word=machine%20learning&amp;sense=auto">word2sense phrase search</a>.</p>

<p>What I find much more interesting is that words form a linear space. In particular, a zero vector represent a totally uncharacteristic word, occurring with every other word at the random chance level (as its scalar product with every word is zero, so is its PMI).</p>

<p>It is one of the reasons why for vector similarity people often use cosine distance, i.e.</p>

\[\frac{\vec{v}_a \cdot \vec{v}_b}{\vert \vec{v}_a \vert \vert \vec{v}_b \vert}.\]

<p>That is, it puts emphasis on the direction in which a given word co-occurs with other words, rather than the strength of this effect.</p>

<h2 id="analogies-and-linear-space">Analogies and linear space<a class="anchorjs-link " href="#analogies-and-linear-space" aria-label="Anchor link for: analogies and linear space" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>If we want to make word analogies (<em>a is to b is as A is to B</em>), one may argue that in can be expressed as an equality of conditional probability ratios</p>

\[\frac{P(w|a)}{P(w|b)} = \frac{P(w|A)}{P(w|B)}\]

<p>for every word <em>w</em>. Sure, it looks (and is!) a questionable assumption, but it is still the best thing we can do with conditional probability. I will not try defending this formulation - I will show its equivalent formulations.</p>

<p>For example, if we want to say <em>dog is to puppy as cat is to kitten</em>, we expect that if e.g. word <em>nice</em> co-occurs with both <em>dog</em> and <em>cat</em> (likely with different frequencies), then it co-occurs with <em>puppy</em> and <em>kitten</em> by the same factor.
It appears it is true, with the factor of two favoring the cubs - compare <a href="https://books.google.com/ngrams/graph?content=nice+cat%2Cnice+kitten%2Cnice+dog%2Cnice+puppy&amp;year_start=1960&amp;year_end=2008&amp;corpus=0&amp;smoothing=5&amp;share=&amp;direct_url=t1%3B%2Cnice%20cat%3B%2Cc0%3B.t1%3B%2Cnice%20kitten%3B%2Cc0%3B.t1%3B%2Cnice%20dog%3B%2Cc0%3B.t1%3B%2Cnice%20puppy%3B%2Cc0">pairs</a> to <a href="https://books.google.com/ngrams/graph?content=cat%2Ckitten%2Cdog%2Cpuppy&amp;year_start=1960&amp;year_end=2008&amp;corpus=0&amp;smoothing=5&amp;share=&amp;direct_url=t1%3B%2Ccat%3B%2Cc0%3B.t1%3B%2Ckitten%3B%2Cc0%3B.t1%3B%2Cdog%3B%2Cc0%3B.t1%3B%2Cpuppy%3B%2Cc0">words</a> from <a href="https://books.google.com/ngrams">Google Books Ngram Viewer</a> (while n-grams look only at adjacent words, they can be some sort of approximation).</p>

<p>By proposing ratios for word analogies we implicitly assume that the probabilities of words can be factorized with respect to different dimensions of a word. For the discussed case it would be:</p>

\[P(w\vert dog) = f(w\vert species=dog) \times f(w\vert age=adult) \times P(w\vert is\_a\_pet) \\
P(w\vert puppy) = f(w\vert species=dog) \times f(w\vert age=cub) \times P(w\vert is\_a\_pet) \\
P(w\vert cat) = f(w\vert species=cat) \times f(w\vert age=adult) \times P(w\vert is\_a\_pet) \\
P(w\vert kitten) = f(w\vert species=cat) \times f(w\vert age=cub) \times P(w\vert is\_a\_pet)\]

<p>So, in particular:</p>

\[\frac{P(w|dog)}{P(w|puppy)} = \frac{f(w\vert age=adult)}{f(w\vert age=cub)} = \frac{P(w|cat)}{P(w|kitten)}.\]

<p>How does the equality of conditional probability ratios translate to the word vectors?
If we express it as mutual information (again, <em>P(w)</em> and logarithms) we get</p>

\[\vec{v}_w \cdot \vec{v}_a - \vec{v}_w \cdot \vec{v}_b = \vec{v}_w \cdot \vec{v}_A - \vec{v}_w \cdot \vec{v}_B,\]

<p>which is the same as</p>

\[\vec{v}_w \cdot \left( \vec{v}_a - \vec{v}_b - \vec{v}_A + \vec{v}_B \right) = 0.\]

<p>Again, if we want it to hold for any word <em>w</em>, this vector difference needs to be zero.</p>

<p>We can use analogies for meaning (e.g. changing gender with vectors), grammar (e.g. changing tenses) or other analogies (e.g. cities into their zip codes).
It seems that analogies are not only a computational trick - we may actually use them to think all the time, see:</p>

<ul>
  <li>George Lakoff, Mark Johnson, <a href="https://www.amazon.com/Metaphors-We-Live-George-Lakoff/dp/0226468011">Metaphors We Live By</a> (1980)</li>
  <li>and <a href="http://web.archive.org/web/20080718021721/http://cogsci.berkeley.edu/lakoff/metaphors/">their list of conceptual metaphors in English (webarchive)</a>, in particular look for <em>X is Up</em>, plotted below:</li>
</ul>

<p><img src="/imgs/2017-01-06/word2viz-up-down-metaphors.png" alt=""></p>

<h2 id="differences-and-projections">Differences and projections<a class="anchorjs-link " href="#differences-and-projections" aria-label="Anchor link for: differences and projections" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<blockquote>
  <p><code class="language-plaintext highlighter-rouge">woman - man = female - male = she - he</code>, so <code class="language-plaintext highlighter-rouge">wo = fe = s</code> (a joke)</p>
</blockquote>

<p>Difference of words vectors, like</p>

\[\vec{v}_{she} - \vec{v}_{he}\]

<p>are not words vectors by themselves.
However, it is interesting to project a word on this axis.
We can see that the projection</p>

\[\vec{v}_w \cdot \left( \vec{v}_a - \vec{v}_b \right)
= \log\left[ P(w|a) \right] - \log\left[ P(w|b) \right]\]

<p>is exactly a relative occurrence of a word within different contexts.</p>

<p>Bear in mind that when we want to look at common aspects of a word it is more natural to average two vectors rather than take their sum. While people use it interchangeably, it only works because cosine distance ignores the absolute vector length. So, for a gender neutral pronoun use \((\vec{v}_{she} + \vec{v}_{he})/2\) rather than their sum.</p>

<p>Just looking at the word co-locations can give interesting results, look at these artistic projects - <a href="http://www.chrisharrison.net/index.php/Visualizations/WordSpectrum">Word Spectrum</a> and <a href="http://www.chrisharrison.net/index.php/Visualizations/WordAssociations">Word Associations</a> from Visualizing Google’s Bi-Gram Data by <a href="http://www.chrisharrison.net/">Chris Harrison</a>.</p>

<h2 id="i-want-to-play">I want to play!<a class="anchorjs-link " href="#i-want-to-play" aria-label="Anchor link for: i want to play" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>If you want <strong>explore</strong>, there is <a href="https://lamyiowce.github.io/word2viz/">Word2viz</a> by Julia Bazińska.
You can choose between one of several pre-defined plots, or create one from scratch (choosing words and projections).
I’ve just realized that Google Research released a tool for that as well:
<a href="https://research.googleblog.com/2016/12/open-sourcing-embedding-projector-tool.html">Open sourcing the Embedding Projector: a tool for visualizing high dimensional data - Google Research blog</a> (and the actual live demo: <a href="http://projector.tensorflow.org/">Embedding Projector</a>).</p>

<p>If you want to use <strong>pre-trained vectors</strong>, see <a href="http://nlp.stanford.edu/projects/glove/">Stanford GloVe</a> or <a href="https://code.google.com/archive/p/word2vec/">Google word2vec</a> download sections.
Some examples in Jupyter Notebooks are in our playground <a href="https://github.com/lamyiowce/word2viz">github.com/lamyiowce/word2viz</a> (warning: raw state, look at them at your own risk).</p>

<p>If you want to train it on your <strong>own dataset</strong>, use a Python library <a href="https://radimrehurek.com/gensim/">gensim: Topic modelling for humans</a>.
Often some preprocessing is needed, especially look at <a href="https://explosion.ai/blog/sense2vec-with-spacy">Sense2vec with spaCy and Gensim</a> by Matthew Honnibal.</p>

<p>If you want to create it <strong>from scratch</strong>, the most convenient way is to start with <a href="https://www.tensorflow.org/versions/r0.11/tutorials/word2vec/index.html">Vector Representations of Words - TensorFlow Tutorial</a> (see also a respective <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/5_word2vec.ipynb">Jupyter Notebook from Udacity Course</a>).</p>

<p>If you want to <strong>learn</strong> how it works, I recommend the following materials:</p>

<ul>
  <li><a href="http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/">A Word is Worth a Thousand Vectors</a> by Chris Moody</li>
  <li>Daniel Jurafsky, James H. Martin, <a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a> (2015):
    <ul>
      <li><a href="https://web.stanford.edu/~jurafsky/slp3/15.pdf">Chapter 15: Vector Semantics</a> (and <a href="https://web.stanford.edu/~jurafsky/slp3/slides/vector1.pdf">slides</a>)</li>
      <li><a href="https://web.stanford.edu/~jurafsky/slp3/16.pdf">Chapter 16: Semantics with Dense Vectors</a> (and <a href="https://web.stanford.edu/~jurafsky/slp3/slides/vector2.pdf">slides</a>)</li>
    </ul>
  </li>
  <li><a href="http://web.stanford.edu/class/linguist236/materials/ling236-handout-05-09-vsm.pdf">Distributional approaches to word meanings</a> from <a href="http://web.stanford.edu/class/linguist236/">Seminar: Representations of Meaning course at Stanford by Noah D. Goodman and Christopher Potts</a></li>
  <li><a href="http://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a> and paper
    <ul>
      <li>Jeffrey Pennington, Richard Socher, Christopher D. Manning, <a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> (2014)</li>
      <li>it’s great, except for its claims for greatness, see: <a href="https://rare-technologies.com/making-sense-of-word2vec">GloVe vs word2vec</a> by Radim Rehurek</li>
    </ul>
  </li>
  <li><a href="http://norvig.com/chomsky.html">On Chomsky and the Two Cultures of Statistical Learning</a> by Peter Norvig</li>
</ul>

<p><img src="/imgs/2017-01-06/queen-julia-vectors.jpg" alt=""></p>

<blockquote>
  <p>Julia Bazińska, at the rooftop garden of the <a href="https://en.wikipedia.org/wiki/Warsaw_University_Library">Warsaw University Library</a> - the building in which we worked</p>
</blockquote>

<h2 id="technicalities">Technicalities<a class="anchorjs-link " href="#technicalities" aria-label="Anchor link for: technicalities" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>I tried to give some insight into algorithms transforming words into vectors. Every practical approach needs a bit more tweaking. Here are a few clarifications:</p>

<ul>
  <li>word2vec is not a single task or algorithm; popular ones are:
    <ul>
      <li>Skip-Gram Negative-Sampling (implicit compression of PMI),</li>
      <li>Skip-Gram Noise-Contrastive Training (implicit compression of conditional probability),</li>
      <li>GloVe (explicit compression of co-occurrences),</li>
    </ul>
  </li>
  <li>while <em>word</em> and <em>context</em> are essentially the same thing (both are words), they are being probed and treated differently (to account for different word frequencies),</li>
  <li>there are two sets of vectors (each word has two vectors, one for word and the other - for context),</li>
  <li>as any practical dataset of occurrences would contain PMI \(-\infty\) for some pairs, in most cases positive pointwise mutual information (PPMI) is being used instead,</li>
  <li>often pre-processing is needed (e.g. to catch phrases like <em>machine learning</em> or to distinguish words with two separate meanings),</li>
  <li>linear space of meaning is a disputed concept,</li>
  <li>all results are a function of the data we used to feed our algorithm, not objective truth; so it is easy to get stereotypes like <code class="language-plaintext highlighter-rouge">doctor - man + woman = nurse</code>.</li>
</ul>

<p>For further reading I recommend:</p>

<ul>
  <li><a href="https://www.quora.com/How-does-word2vec-work">How does word2vec work?</a> by Omer Levy</li>
  <li>Omer Levy, Yoav Goldberg, <a href="https://levyomer.files.wordpress.com/2014/09/neural-word-embeddings-as-implicit-matrix-factorization.pdf">Neural Word Embeddings as Implicit Matrix Factorization</a>, NIPS 2014</li>
  <li>with a caveat: <a href="http://building-babylon.net/2016/05/12/skipgram-isnt-matrix-factorisation/">Skipgram isn’t Matrix Factorisation</a> by Benjamin Wilson</li>
  <li><a href="http://nlpers.blogspot.ie/2016/06/language-bias-and-black-sheep.html">Language bias and black sheep</a></li>
  <li><a href="https://freedom-to-tinker.com/2016/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/">Language necessarily contains human biases, and so will machines trained on language corpora</a> by Arvind Narayanan</li>
  <li><a href="http://www.offconvex.org/2016/02/14/word-embeddings-2/">Word Embeddings: Explaining their properties</a> - on word analogies by Sanjeev Arora</li>
  <li>Tal Linzen, <a href="https://arxiv.org/abs/1606.07736">Issues in evaluating semantic spaces using word analogies</a>, arXiv:1606.07736</li>
  <li><strong>EDIT</strong> (Feb 2018): Alex Gittens, Dimitris Achlioptas, Michael W. Mahoney, <a href="http://www.aclweb.org/anthology/P/P17/P17-1007.pdf">Skip-Gram – Zipf + Uniform = Vector Additivity</a></li>
</ul>

<h2 id="some-backstory">Some backstory<a class="anchorjs-link " href="#some-backstory" aria-label="Anchor link for: some backstory" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>I got interested in word2vec and related techniques for my general interest in machine learning and for my general appreciations of:</p>

<ul>
  <li>matrix factorization,</li>
  <li>pointwise mutual information,</li>
  <li>conceptual metaphors,</li>
  <li>simple techniques mimicking human cognition.</li>
</ul>

<p>I had an motivation to learn more on the subject as I was tutoring Julia Bazińska during a two-week summer internship at <a href="http://www.delab.uw.edu.pl/">DELab, University of Warsaw</a>, supported by the Polish Children’s Fund. See also my blog posts:</p>

<ul>
  <li><a href="http://crastina.se/gifted-children-in-poland-by-piotr-migdal/">Helping exceptionally gifted children in Poland</a> - on the Polish Children’s Fund</li>
  <li><a href="http://p.migdal.pl/2016/02/09/d3js-icm-kfnrd.html">D3.js workshop at ICM for KFnrD</a> in Jan 2016, where it all started</li>
</ul>

<h2 id="thanks">Thanks<a class="anchorjs-link " href="#thanks" aria-label="Anchor link for: thanks" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>This draft benefited from feedback from <a href="https://github.com/grzegorz225/">Grzegorz Uriasz</a> (what’s simple and what isn’t), <a href="http://goodsexlifestyle.com/">Sarah Martin</a> (readability and grammar remarks). I want to especially thank <a href="https://levyomer.wordpress.com/">Levy Omer</a> for pointing to weak points (and shady assumptions) of word vector arithmetics.</p>

<h2 id="edit">EDIT<a class="anchorjs-link " href="#edit" aria-label="Anchor link for: edit" data-anchorjs-icon="" style="font: 1em / 1 anchorjs-icons; padding-left: 0.375em;"></a></h2>

<p>It got some popularity, including ~20k views in the first day, and being tweeted by <a href="https://twitter.com/stanfordnlp/status/818881718077661184">the authors of GloVe at Stanford</a> and <a href="https://twitter.com/kaggle/status/819258895235424258">Kaggle</a>.</p>

<p>Though, what I like the most is to see how people interact with it:</p>

<ul>
  <li><a href="https://twitter.com/cesifoti/status/818672972743450624">artistic-scientific impulsive-analytical</a> by Cesar Hidalgo from MIT Media Lab</li>
  <li><a href="http://imgur.com/3FzX81i">good-evil unlawful-lawful and AD&amp;D classes</a> from HN comment thread :)</li>
</ul>

  </div>

  
  <div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; width: 0px; height: 0px;"><div></div></div></div>
<script>(function (d, s, id) {
    var js, fjs = d.getElementsByTagName(s)[0];
    if (d.getElementById(id)) return;
    js = d.createElement(s); js.id = id;
    js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.5&appId=262622227112392";
    fjs.parentNode.insertBefore(js, fjs);
  }(document, 'script', 'facebook-jssdk'));</script>

<div class="social">

  <span></span>
  <br>
  <a href="https://twitter.com/share" class="twitter-share-button" data-text="king - man + woman is queen; but why?" data-via="pmigdal">Tweet</a><br>
  <script>!function (d, s, id) { var js, fjs = d.getElementsByTagName(s)[0], p = /^http:/.test(d.location) ? 'http' : 'https'; if (!d.getElementById(id)) { js = d.createElement(s); js.id = id; js.src = p + '://platform.twitter.com/widgets.js'; fjs.parentNode.insertBefore(js, fjs); } }(document, 'script', 'twitter-wjs');</script>

  <div class="fb-like fb_iframe_widget" data-href="/2017/01/06/king-man-woman-queen-why.html" data-layout="standard" data-action="like" data-show-faces="true" data-share="true" fb-xfbml-state="parsed" fb-iframe-plugin-query="action=like&amp;app_id=262622227112392&amp;container_width=740&amp;href=https%3A%2F%2Fp.migdal.pl%2F2017%2F01%2F06%2Fking-man-woman-queen-why.html&amp;layout=standard&amp;locale=en_US&amp;sdk=joey&amp;share=true&amp;show_faces=true"><span style="vertical-align: top; width: 0px; height: 0px; overflow: hidden;"><iframe name="f23efc4a5e23684" width="1000px" height="1000px" data-testid="fb:like Facebook Social Plugin" title="fb:like Facebook Social Plugin" frameborder="0" allowtransparency="true" allowfullscreen="true" scrolling="no" allow="encrypted-media" src="https://www.facebook.com/v2.5/plugins/like.php?action=like&amp;app_id=262622227112392&amp;channel=https%3A%2F%2Fstaticxx.facebook.com%2Fx%2Fconnect%2Fxd_arbiter%2F%3Fversion%3D46%23cb%3Df3119f3cf46168%26domain%3Dp.migdal.pl%26is_canvas%3Dfalse%26origin%3Dhttps%253A%252F%252Fp.migdal.pl%252Ff734aaeacfb4%26relation%3Dparent.parent&amp;container_width=740&amp;href=https%3A%2F%2Fp.migdal.pl%2F2017%2F01%2F06%2Fking-man-woman-queen-why.html&amp;layout=standard&amp;locale=en_US&amp;sdk=joey&amp;share=true&amp;show_faces=true" style="border: none; visibility: hidden;"></iframe></span></div><br>

  <a href="javascript:window.location=%22http://news.ycombinator.com/submitlink?u=%22+encodeURIComponent(document.location)+%22&amp;t=%22+encodeURIComponent(&quot;king - man + woman is queen; but why?&quot;)">
    <span class="icon">
      <img src="/imgs/global/hn.gif" width="16" height="16">
    </span>
    HN Submission/Discussion
  </a><br>

  <a href="http://quantumgame.us9.list-manage.com/subscribe?u=5e3bdb13b61924c4b0ec92fba&amp;id=6c72ded7d2">
    <span class="icon">
      <img src="/imgs/global/mailchimp.png" width="16" height="16">
    </span>
    Get notified about new posts via email (MailChimp)
  </a><br>
</div>

<script async="" defer="" src="https://buttons.github.io/buttons.js"></script>
  

  

  <script src="/lib/anchor.min.js"></script>
<script>
  anchors.add().remove(".post-title");
</script>

</article>
    </div>
  </div>

  <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-col-wrapper">

      <div class="footer-col footer-col-1">
        <img src="https://p.migdal.pl/imgs/global/piotr_migdal_2016_london_300sq.jpg" alt="Piotr Migdał - as of 2013" width="150px" height="150px">
      </div>

      <div class="footer-col footer-col-2">
        <ul class="contact-list">
          <li><a href="mailto:pmigdal@gmail.com">pmigdal@gmail.com</a></li>
          <li><a href="https://p.migdal.pl">https://p.migdal.pl</a></li>
        </ul>
        <ul class="social-media-list">
          <li>
            <a href="https://github.com/stared"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path></svg>
</span><span class="username">stared</span></a>
          </li>
          <li>
            <a href="https://twitter.com/pmigdal"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path></svg>
</span><span class="username">pmigdal</span></a>
          </li>
          <li>
            <a href="https://pinboard.in/u:pmigdal/">
              <span class="icon icon--pinboard"><svg viewBox="0 0 25 25">
  <polygon xmlns="http://www.w3.org/2000/svg" fill="#828282" points="13.908,14.85 9.211,19.547 9.961,15.412 3.571,7.706 0,7.894 7.895,0 7.895,3.007 15.412,9.773  20.111,8.647 15.227,13.721 25,24.436 "></polygon>
</svg>
</span>
            </a>
            <a href="https://linkedin.com/in/piotrmigdal">
              <span class="fa fa-linkedin-square" style="color:#828282"></span>
            </a>
            <a href="https://stackexchange.com/users/506817/piotr-migdal">
              <span class="fa fa-stack-overflow" style="color:#828282"></span>
            </a>
          </li>
          <li>
            <span></span>
          </li>
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Piotr Migdał - founder of <a href="https://quantumflytrap.com">Quantum Flytrap</a> with Klem Jankiewicz.
          Previously: an independent data science consultant.
          PhD in quantum physics. Based in Warsaw, Poland.
          Believing in <a href="http://crastina.se/theres-no-projects-like-side-projects/">side projects</a>, active in
          <a href="https://warsztatywww.pl/article/en-indie-camp-for-hs-geeks/">gifted education</a>, writing and
          developing open source projects.

        </p><p>Python for data science, TypeScript for interfaces, Rust for performance.</p>
      </div>

    </div>

  </div>

  <script async="" defer="" src="https://buttons.github.io/buttons.js"></script>

</footer>
  <script>
  (function (i, s, o, g, r, a, m) {
    i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date(); a = s.createElement(o),
      m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
  })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

  ga('create', 'UA-73544535-1', 'auto');
  ga('send', 'pageview');

</script>



<iframe scrolling="no" frameborder="0" allowtransparency="true" src="https://platform.twitter.com/widgets/widget_iframe.f88235f49a156f8b4cab34c7bc1a0acc.html?origin=https%3A%2F%2Fp.migdal.pl" title="Twitter settings iframe" style="display: none;"></iframe></body></html>
