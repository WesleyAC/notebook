{
    "archive_path": "archive/1632693002.808384",
    "base_url": "p.migdal.pl/2017/01/06/king-man-woman-queen-why.html",
    "basename": "king-man-woman-queen-why.html",
    "bookmarked_date": "2021-09-26 21:50",
    "canonical": {
        "archive_org_path": "https://web.archive.org/web/p.migdal.pl/2017/01/06/king-man-woman-queen-why.html",
        "dom_path": "output.html",
        "favicon_path": "favicon.ico",
        "git_path": "git/",
        "google_favicon_path": "https://www.google.com/s2/favicons?domain=p.migdal.pl",
        "headers_path": "headers.json",
        "index_path": "index.html",
        "media_path": "media/",
        "mercury_path": "mercury/content.html",
        "pdf_path": "output.pdf",
        "readability_path": "readability/content.html",
        "screenshot_path": "screenshot.png",
        "singlefile_path": "singlefile.html",
        "warc_path": "warc/",
        "wget_path": null
    },
    "domain": "p.migdal.pl",
    "extension": "html",
    "hash": "WQC7DBPF2YFYF6K751CJ",
    "history": {
        "archive_org": [
            {
                "cmd": [
                    "curl",
                    "--silent",
                    "--location",
                    "--compressed",
                    "--head",
                    "--max-time",
                    "90",
                    "--user-agent",
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/0.6.2 (+https://github.com/ArchiveBox/ArchiveBox/) curl/curl 7.76.1 (x86_64-pc-linux-gnu)",
                    "https://web.archive.org/save/https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
                ],
                "cmd_version": "curl 7.76.1 (x86_64-pc-linux-gnu)",
                "end_ts": "2021-09-26T21:50:57.500037+00:00",
                "index_texts": null,
                "output": "https://web.archive.org/web/20210926215046/https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:37.793722+00:00",
                "status": "succeeded"
            }
        ],
        "dom": [
            {
                "cmd": [
                    "chromium-browser",
                    "--headless",
                    "--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/{VERSION} (+https://github.com/ArchiveBox/ArchiveBox/)",
                    "--window-size=1440,2000",
                    "--timeout=90000",
                    "--dump-dom",
                    "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
                ],
                "cmd_version": "Chromium 93.0.4577.82",
                "end_ts": "2021-09-26T21:50:21.211576+00:00",
                "index_texts": null,
                "output": "output.html",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:19.647298+00:00",
                "status": "succeeded"
            }
        ],
        "favicon": [
            {
                "cmd": [
                    "curl",
                    "--silent",
                    "--location",
                    "--compressed",
                    "--max-time",
                    "90",
                    "--output",
                    "favicon.ico",
                    "--user-agent",
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/0.6.2 (+https://github.com/ArchiveBox/ArchiveBox/) curl/curl 7.76.1 (x86_64-pc-linux-gnu)",
                    "https://www.google.com/s2/favicons?domain=p.migdal.pl"
                ],
                "cmd_version": "curl 7.76.1 (x86_64-pc-linux-gnu)",
                "end_ts": "2021-09-26T21:50:03.937301+00:00",
                "index_texts": null,
                "output": "favicon.ico",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:03.588432+00:00",
                "status": "succeeded"
            }
        ],
        "git": [],
        "headers": [
            {
                "cmd": [
                    "curl",
                    "--silent",
                    "--location",
                    "--compressed",
                    "--head",
                    "--max-time",
                    "90",
                    "--user-agent",
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/0.6.2 (+https://github.com/ArchiveBox/ArchiveBox/) curl/curl 7.76.1 (x86_64-pc-linux-gnu)",
                    "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
                ],
                "cmd_version": "curl 7.76.1 (x86_64-pc-linux-gnu)",
                "end_ts": "2021-09-26T21:50:04.128035+00:00",
                "index_texts": null,
                "output": "headers.json",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:03.953150+00:00",
                "status": "succeeded"
            }
        ],
        "media": [],
        "mercury": [
            {
                "cmd": [
                    "/home/wesleyac/code/notebook/data/archivebox/node_modules/@postlight/mercury-parser/cli.js",
                    "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
                ],
                "cmd_version": "1.0.0",
                "end_ts": "2021-09-26T21:50:37.778237+00:00",
                "index_texts": null,
                "output": "mercury",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:32.291307+00:00",
                "status": "succeeded"
            }
        ],
        "pdf": [],
        "readability": [
            {
                "cmd": [
                    "/home/wesleyac/code/notebook/data/archivebox/node_modules/readability-extractor/readability-extractor",
                    "/run/user/1000/tmpkp195pz2"
                ],
                "cmd_version": "0.0.3",
                "end_ts": "2021-09-26T21:50:32.263867+00:00",
                "index_texts": [
                    "\n    Intro\n\nword2vec is an algorithm that transforms words into vectors, so that words with similar meaning end up laying close to each other. Moreover, it allows us to use vector arithmetics to work with analogies, for example the famous king - man + woman = queen.\n\nI will try to explain how it works, with special emphasis on the meaning of vector differences, at the same time omitting as many technicalities as possible.\n\nIf you would rather explore than read, here is an interactive exploration by my mentee Julia Bazi\u0144ska, now a freshman of computer science at the University of Warsaw:\n\n\n  Word2viz by using GloVe pre-trained vectors (it takes 30MB to load - please be patient)\n\n\n\n\nCounts, coincidences and meaning\n\n\n  I love letter co-occurrence in the word co-occurrence.\n\n\nSometimes a seemingly naive technique gives powerful results. It turns out that merely looking at word coincidences, while ignoring all grammar and context, can provide us insight into the meaning of a word.\nConsider this sentence:\n\n\n  A small, fluffy roosety climbed a tree.\n\n\nWhat\u2019s a roosety? I would say that something like a squirrel, since the two words can be easily interchanged. Such reasoning is called the distributional hypothesis and can be summarized as:\n\n\n  a word is characterized by the company it keeps - John Rupert Firth\n\n\nIf we want to teach it to a computer, the simplest, approximated approach is making it look only at word pairs.\nLet P(a|b) be the conditional probability that given a word b there is a word a within a short distance (let\u2019s say - being spaced by no more that 2 words).\nThen we claim that two words a and b are similar if\n\nP(w|a)=P(w|b)\n\nfor every word w.\nIn other words, if we have this equality, no matter if there is word a or b, all other words occur with the same frequency.\n\nEven simple word counts, compared by source, can give interesting results, e.g. that in lyrics of metal songs words (cries, eternity or ashes are popular, while words particularly or approximately are not, well, particularly common), see Heavy Metal and Natural Language Processing.\nSee also Gender Roles with Text Mining and N-grams by Julia Silge.\n\nLooking at co-occurrences can provide much more information. For example, one of my projects, TagOverflow, gives insight into structure of programming, based only on the usage of tags on Stack Overflow.\nIt also shows that I am in love with pointwise mutual information, which brings us to the next point.\n\n\n\nPointwise mutual information and compression\n\n\n  The Compressors: View cognition as compression. Compressed sensing, approximate matrix factorization - The (n) Cultures of Machine Learning - HN discussion\n\n\nIn principle, we can compute P(a|b) for every word pair.\nBut even with a small dictionary of 100 000 words (bear in mind that we need to keep all declinations, proper names and things which are not in official dictionaries, yet are in use) keeping track of all pairs would require 8 gigabytes of space.\n\nOften instead of working with conditional probabilities, we use the pointwise mutual information (PMI), defined as:\n\nPMI(a,b)=log\u2061[P(a,b)P(a)P(b)]=log\u2061[P(a|b)P(a)].\n\nIts direct interpretation is how much more likely we get a pair than if it were at random.\nThe logarithm makes it easier to work with words appearing at frequencies of different orders of magnitude.\nWe can approximate PMI as a scalar product:\n\nPMI(a,b)=v\u2192a\u22c5v\u2192b,\n\nwhere v\u2192i are vectors, typically of 50-300 dimensions.\n\nAt the first glance it may be strange that all words can be compressed to a space of much smaller dimensionality. But there are words that can be trivially interchanged (e.g. John to Peter) and there is a lot of structure in general.\n\nThe fact that this compression is lossy may give it an advantage, as it can discover patterns rather than only memorize each pair.\nFor example, in recommendation systems for movie ratings, each rating is approximated by a scalar product of two vectors - a movie\u2019s content and a user\u2019s preference. This is used to predict scores for as yet unseen movies, see Matrix Factorization with TensorFlow - Katherine Bailey.\n\nWord similarity and vector closeness\n\n\n  Stock Market \u2248 Thermometer - Five crazy abstractions my word2vec model just did\n\n\nLet us start with the simple stuff - showing word similarity in a vector space.\nThe condition that P(w|a)=P(w|b) is equivalent to\n\nPMI(w,a)=PMI(w,b),\n\nby dividing both sides by P(w) and taking their logarithms.\nAfter expressing PMI with vector products, we get\n\nv\u2192w\u22c5v\u2192a=v\u2192w\u22c5v\u2192b\n\nv\u2192w\u22c5(v\u2192a\u2212v\u2192b)=0\n\nIf it needs to work for every v\u2192w, then\n\nv\u2192a=v\u2192b.\n\nOf course, in every practical case we won\u2019t get an exact equality, just words being close to each other. Words close in this space are often synonyms (e.g. happy and delighted), antonyms (e.g. good and evil) or other easily interchangeable words (e.g. yellow and blue).\nIn particular most of opposing ideas (e.g. religion and atheism) will have similar context. If you want to play with that, look at this word2sense phrase search.\n\nWhat I find much more interesting is that words form a linear space. In particular, a zero vector represent a totally uncharacteristic word, occurring with every other word at the random chance level (as its scalar product with every word is zero, so is its PMI).\n\nIt is one of the reasons why for vector similarity people often use cosine distance, i.e.\n\nv\u2192a\u22c5v\u2192b|v\u2192a||v\u2192b|.\n\nThat is, it puts emphasis on the direction in which a given word co-occurs with other words, rather than the strength of this effect.\n\nAnalogies and linear space\n\nIf we want to make word analogies (a is to b is as A is to B), one may argue that in can be expressed as an equality of conditional probability ratios\n\nP(w|a)P(w|b)=P(w|A)P(w|B)\n\nfor every word w. Sure, it looks (and is!) a questionable assumption, but it is still the best thing we can do with conditional probability. I will not try defending this formulation - I will show its equivalent formulations.\n\nFor example, if we want to say dog is to puppy as cat is to kitten, we expect that if e.g. word nice co-occurs with both dog and cat (likely with different frequencies), then it co-occurs with puppy and kitten by the same factor.\nIt appears it is true, with the factor of two favoring the cubs - compare pairs to words from Google Books Ngram Viewer (while n-grams look only at adjacent words, they can be some sort of approximation).\n\nBy proposing ratios for word analogies we implicitly assume that the probabilities of words can be factorized with respect to different dimensions of a word. For the discussed case it would be:\n\nP(w|dog)=f(w|species=dog)\u00d7f(w|age=adult)\u00d7P(w|is_a_pet)P(w|puppy)=f(w|species=dog)\u00d7f(w|age=cub)\u00d7P(w|is_a_pet)P(w|cat)=f(w|species=cat)\u00d7f(w|age=adult)\u00d7P(w|is_a_pet)P(w|kitten)=f(w|species=cat)\u00d7f(w|age=cub)\u00d7P(w|is_a_pet)\n\nSo, in particular:\n\nP(w|dog)P(w|puppy)=f(w|age=adult)f(w|age=cub)=P(w|cat)P(w|kitten).\n\nHow does the equality of conditional probability ratios translate to the word vectors?\nIf we express it as mutual information (again, P(w) and logarithms) we get\n\nv\u2192w\u22c5v\u2192a\u2212v\u2192w\u22c5v\u2192b=v\u2192w\u22c5v\u2192A\u2212v\u2192w\u22c5v\u2192B,\n\nwhich is the same as\n\nv\u2192w\u22c5(v\u2192a\u2212v\u2192b\u2212v\u2192A+v\u2192B)=0.\n\nAgain, if we want it to hold for any word w, this vector difference needs to be zero.\n\nWe can use analogies for meaning (e.g. changing gender with vectors), grammar (e.g. changing tenses) or other analogies (e.g. cities into their zip codes).\nIt seems that analogies are not only a computational trick - we may actually use them to think all the time, see:\n\n\n  George Lakoff, Mark Johnson, Metaphors We Live By (1980)\n  and their list of conceptual metaphors in English (webarchive), in particular look for X is Up, plotted below:\n\n\n\n\nDifferences and projections\n\n\n  woman - man = female - male = she - he, so wo = fe = s (a joke)\n\n\nDifference of words vectors, like\n\nv\u2192she\u2212v\u2192he\n\nare not words vectors by themselves.\nHowever, it is interesting to project a word on this axis.\nWe can see that the projection\n\nv\u2192w\u22c5(v\u2192a\u2212v\u2192b)=log\u2061[P(w|a)]\u2212log\u2061[P(w|b)]\n\nis exactly a relative occurrence of a word within different contexts.\n\nBear in mind that when we want to look at common aspects of a word it is more natural to average two vectors rather than take their sum. While people use it interchangeably, it only works because cosine distance ignores the absolute vector length. So, for a gender neutral pronoun use (v\u2192she+v\u2192he)/2 rather than their sum.\n\nJust looking at the word co-locations can give interesting results, look at these artistic projects - Word Spectrum and Word Associations from Visualizing Google\u2019s Bi-Gram Data by Chris Harrison.\n\nI want to play!\n\nIf you want explore, there is Word2viz by Julia Bazi\u0144ska.\nYou can choose between one of several pre-defined plots, or create one from scratch (choosing words and projections).\nI\u2019ve just realized that Google Research released a tool for that as well:\nOpen sourcing the Embedding Projector: a tool for visualizing high dimensional data - Google Research blog (and the actual live demo: Embedding Projector).\n\nIf you want to use pre-trained vectors, see Stanford GloVe or Google word2vec download sections.\nSome examples in Jupyter Notebooks are in our playground github.com/lamyiowce/word2viz (warning: raw state, look at them at your own risk).\n\nIf you want to train it on your own dataset, use a Python library gensim: Topic modelling for humans.\nOften some preprocessing is needed, especially look at Sense2vec with spaCy and Gensim by Matthew Honnibal.\n\nIf you want to create it from scratch, the most convenient way is to start with Vector Representations of Words - TensorFlow Tutorial (see also a respective Jupyter Notebook from Udacity Course).\n\nIf you want to learn how it works, I recommend the following materials:\n\n\n  A Word is Worth a Thousand Vectors by Chris Moody\n  Daniel Jurafsky, James H. Martin, Speech and Language Processing (2015):\n    \n      Chapter 15: Vector Semantics (and slides)\n      Chapter 16: Semantics with Dense Vectors (and slides)\n    \n  \n  Distributional approaches to word meanings from Seminar: Representations of Meaning course at Stanford by Noah D. Goodman and Christopher Potts\n  GloVe: Global Vectors for Word Representation and paper\n    \n      Jeffrey Pennington, Richard Socher, Christopher D. Manning, GloVe: Global Vectors for Word Representation (2014)\n      it\u2019s great, except for its claims for greatness, see: GloVe vs word2vec by Radim Rehurek\n    \n  \n  On Chomsky and the Two Cultures of Statistical Learning by Peter Norvig\n\n\n\n\n\n  Julia Bazi\u0144ska, at the rooftop garden of the Warsaw University Library - the building in which we worked\n\n\nTechnicalities\n\nI tried to give some insight into algorithms transforming words into vectors. Every practical approach needs a bit more tweaking. Here are a few clarifications:\n\n\n  word2vec is not a single task or algorithm; popular ones are:\n    \n      Skip-Gram Negative-Sampling (implicit compression of PMI),\n      Skip-Gram Noise-Contrastive Training (implicit compression of conditional probability),\n      GloVe (explicit compression of co-occurrences),\n    \n  \n  while word and context are essentially the same thing (both are words), they are being probed and treated differently (to account for different word frequencies),\n  there are two sets of vectors (each word has two vectors, one for word and the other - for context),\n  as any practical dataset of occurrences would contain PMI \u2212\u221e for some pairs, in most cases positive pointwise mutual information (PPMI) is being used instead,\n  often pre-processing is needed (e.g. to catch phrases like machine learning or to distinguish words with two separate meanings),\n  linear space of meaning is a disputed concept,\n  all results are a function of the data we used to feed our algorithm, not objective truth; so it is easy to get stereotypes like doctor - man + woman = nurse.\n\n\nFor further reading I recommend:\n\n\n  How does word2vec work? by Omer Levy\n  Omer Levy, Yoav Goldberg, Neural Word Embeddings as Implicit Matrix Factorization, NIPS 2014\n  with a caveat: Skipgram isn\u2019t Matrix Factorisation by Benjamin Wilson\n  Language bias and black sheep\n  Language necessarily contains human biases, and so will machines trained on language corpora by Arvind Narayanan\n  Word Embeddings: Explaining their properties - on word analogies by Sanjeev Arora\n  Tal Linzen, Issues in evaluating semantic spaces using word analogies, arXiv:1606.07736\n  EDIT (Feb 2018): Alex Gittens, Dimitris Achlioptas, Michael W. Mahoney, Skip-Gram \u2013 Zipf + Uniform = Vector Additivity\n\n\nSome backstory\n\nI got interested in word2vec and related techniques for my general interest in machine learning and for my general appreciations of:\n\n\n  matrix factorization,\n  pointwise mutual information,\n  conceptual metaphors,\n  simple techniques mimicking human cognition.\n\n\nI had an motivation to learn more on the subject as I was tutoring Julia Bazi\u0144ska during a two-week summer internship at DELab, University of Warsaw, supported by the Polish Children\u2019s Fund. See also my blog posts:\n\n\n  Helping exceptionally gifted children in Poland - on the Polish Children\u2019s Fund\n  D3.js workshop at ICM for KFnrD in Jan 2016, where it all started\n\n\nThanks\n\nThis draft benefited from feedback from Grzegorz Uriasz (what\u2019s simple and what isn\u2019t), Sarah Martin (readability and grammar remarks). I want to especially thank Levy Omer for pointing to weak points (and shady assumptions) of word vector arithmetics.\n\nEDIT\n\nIt got some popularity, including ~20k views in the first day, and being tweeted by the authors of GloVe at Stanford and Kaggle.\n\nThough, what I like the most is to see how people interact with it:\n\n\n  artistic-scientific impulsive-analytical by Cesar Hidalgo from MIT Media Lab\n  good-evil unlawful-lawful and AD&D classes from HN comment thread :)\n\n\n  "
                ],
                "output": "readability",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:21.226276+00:00",
                "status": "succeeded"
            }
        ],
        "screenshot": [
            {
                "cmd": [
                    "chromium-browser",
                    "--headless",
                    "--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/{VERSION} (+https://github.com/ArchiveBox/ArchiveBox/)",
                    "--window-size=1440,2000",
                    "--timeout=90000",
                    "--screenshot",
                    "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
                ],
                "cmd_version": "Chromium 93.0.4577.82",
                "end_ts": "2021-09-26T21:50:19.628071+00:00",
                "index_texts": null,
                "output": "screenshot.png",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:16.840490+00:00",
                "status": "succeeded"
            }
        ],
        "singlefile": [
            {
                "cmd": [
                    "/home/wesleyac/code/notebook/data/archivebox/node_modules/single-file/cli/single-file",
                    "--browser-executable-path=chromium-browser",
                    "--browser-args=[\"--headless\", \"--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/{VERSION} (+https://github.com/ArchiveBox/ArchiveBox/)\", \"--window-size=1440,2000\"]",
                    "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html",
                    "singlefile.html"
                ],
                "cmd_version": "0.3.31",
                "end_ts": "2021-09-26T21:50:16.822967+00:00",
                "index_texts": null,
                "output": "singlefile.html",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:04.160100+00:00",
                "status": "succeeded"
            }
        ],
        "title": [
            {
                "cmd": [
                    "curl",
                    "--silent",
                    "--location",
                    "--compressed",
                    "--max-time",
                    "90",
                    "--user-agent",
                    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.61 Safari/537.36 ArchiveBox/0.6.2 (+https://github.com/ArchiveBox/ArchiveBox/) curl/curl 7.76.1 (x86_64-pc-linux-gnu)",
                    "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
                ],
                "cmd_version": "curl 7.76.1 (x86_64-pc-linux-gnu)",
                "end_ts": "2021-09-26T21:50:03.572711+00:00",
                "index_texts": null,
                "output": "king - man + woman is queen; but why?",
                "pwd": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
                "schema": "ArchiveResult",
                "start_ts": "2021-09-26T21:50:03.145618+00:00",
                "status": "succeeded"
            }
        ],
        "wget": []
    },
    "icons": null,
    "is_archived": true,
    "is_static": false,
    "latest": {
        "archive_org": "https://web.archive.org/web/20210926215046/https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html",
        "dom": "output.html",
        "favicon": "favicon.ico",
        "git": null,
        "media": null,
        "pdf": null,
        "screenshot": "screenshot.png",
        "singlefile": "singlefile.html",
        "title": "king - man + woman is queen; but why?",
        "warc": null,
        "wget": null
    },
    "link_dir": "/home/wesleyac/code/notebook/data/archivebox/archive/1632693002.808384",
    "newest_archive_date": "2021-09-26T21:50:37.793722+00:00",
    "num_failures": 0,
    "num_outputs": 9,
    "oldest_archive_date": "2021-09-26T21:50:03.145618+00:00",
    "path": "/2017/01/06/king-man-woman-queen-why.html",
    "schema": "Link",
    "scheme": "https",
    "snapshot_id": "d03ff777-8a92-44c9-94a1-7cc0c746efd9",
    "sources": [
        "/home/wesleyac/code/notebook/data/archivebox/sources/1632693002-import.txt"
    ],
    "tags": null,
    "tags_str": "",
    "timestamp": "1632693002.808384",
    "title": "king - man + woman is queen; but why?",
    "updated": "2021-09-26T21:50:03.141919+00:00",
    "updated_date": "2021-09-26 21:50",
    "url": "https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
}